{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Project_Analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONMIR1PehvYrWFnXbwkhdB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"o16SFzMM637z"},"source":["# Load Packages"]},{"cell_type":"code","metadata":{"id":"vTnq3S8Rl4RQ","executionInfo":{"status":"ok","timestamp":1620477131904,"user_tz":-270,"elapsed":607,"user":{"displayName":"Parisa Yalsavar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHdnQtdpJ7oP45jj0RfM96M-L1Zv_aOmk_JZuWfw=s64","userId":"04215397285859188777"}}},"source":["import pandas as pd\n","import numpy as np\n","import json\n","import re\n","import csv\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","from tqdm import tqdm\n","import ast\n","import os\n","from os import path\n","from pandas.plotting import table\n","from google.colab import output\n","from nltk.stem import PorterStemmer\n","from itertools import chain\n","from collections import Counter\n","import operator\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","import string\n","from google.colab import files\n","output.clear()\n","\n","%matplotlib inline\n","pd.set_option('display.max_colwidth', 300)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8b6LFLCpxS6"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"2uK3BGF0l5EA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItHfv_3il9HP","executionInfo":{"status":"ok","timestamp":1620477137735,"user_tz":-270,"elapsed":1889,"user":{"displayName":"Parisa Yalsavar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHdnQtdpJ7oP45jj0RfM96M-L1Zv_aOmk_JZuWfw=s64","userId":"04215397285859188777"}}},"source":["load_path = '/content/drive/MyDrive/NLP_Project/data/sentence_broken/'\n","load_path2 = '/content/drive/MyDrive/NLP_Project/data/word_broken/'\n","\n","sentence_level_df = pd.read_pickle(load_path + 'sentence_level_df.pkl')\n","# sentence_level_df = sentence_level_df.drop(sentence_level_df.columns[[0]], axis = 1).reset_index(drop=True)\n","\n","word_level_df = pd.read_pickle(load_path2 + 'word_level_df.pkl')\n","# word_level_df = word_level_df.drop(word_level_df.columns[[0]], axis = 1).reset_index(drop=True)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZjLTU7GhLGQ"},"source":["# Parts"]},{"cell_type":"markdown","metadata":{"id":"p7RHCsYz4c_R"},"source":["### Common and non common words between tags"]},{"cell_type":"code","metadata":{"id":"8H3qvHegOquX","executionInfo":{"status":"ok","timestamp":1620477187650,"user_tz":-270,"elapsed":46700,"user":{"displayName":"Parisa Yalsavar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHdnQtdpJ7oP45jj0RfM96M-L1Zv_aOmk_JZuWfw=s64","userId":"04215397285859188777"}}},"source":["individual_distinct_words = []\n","others_ditinct_words = []\n","common_words = []\n","\n","all_groups = word_level_df['Words'].values\n","word_counter = Counter(chain(*all_groups))\n","list_all_words = list(set(word_counter.elements()))\n","\n","for i, group in enumerate(all_groups):\n","  \n","  current_group = set(group)\n","  other_groups = set(np.concatenate(np.delete(all_groups, i, axis = 0)))\n","\n","  print(i)\n","  print(len(current_group))\n","  print(len(other_groups))\n","\n","  individual_distinct = []\n","  others_distinct = []\n","  common = []\n","\n","  for word in list_all_words:\n","    if word in current_group and word in other_groups:\n","      common.append(word)\n","\n","    elif word in current_group:\n","      individual_distinct.append(word)\n","    \n","    elif word in other_groups:\n","      others_distinct.append(word)\n","\n","\n","  print(len(individual_distinct))\n","  print(len(others_distinct))\n","  print(len(common))\n","  \n","  individual_distinct_words.append(individual_distinct)\n","  others_ditinct_words.append(others_distinct)\n","  common_words.append(common)\n","\n","output.clear()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zx49RuxzTrpL"},"source":["for i, value in enumerate(zip(individual_distinct_words, others_ditinct_words, common_words)):\n","  individual_distinct, others_distinct, common = value\n","  genre = word_level_df['Genre'][i]\n","  x = [genre , \"Others\", \"Common\"]\n","  y = [len(individual_distinct), len(others_distinct), len(common)]\n","\n","  plt.figure(figsize=(8,10)) \n","  ax = sns.barplot(x = x, y = y) \n","  ax.set_ylabel('Word Count')\n","  # ax.set_xlabel('Unique Count')\n","  plt.savefig(\"common-chart-\" + genre + \".png\", bbox_inches='tight')\n","  files.download(\"common-chart-\" + genre + \".png\")\n","  plt.show()\n","\n","output.clear()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62sBEaZL_JgB"},"source":["### Top 10 unique words of each tag"]},{"cell_type":"code","metadata":{"id":"HkIl3QUkhDx6"},"source":["all_distinct_counter = []\n","\n","for i, value in enumerate(zip(all_groups, common_words)):\n","  print(i)\n","  current_group, common = all_groups[i], common_words[i]\n","  current_group = current_group.tolist()\n","\n","  current_counter = Counter(current_group)\n","  \n","  common_dict = {word: current_counter[word] for word in common}\n","  print(len(list(set(current_counter.elements()))))\n","\n","  current_counter.subtract(common_dict)\n","  print(len(list(set(current_counter.elements()))))\n","  print(len(individual_distinct_words[i]))\n","  print(current_counter.most_common(10))\n","\n","  all_distinct_counter.append(current_counter)\n","\n","  # plot chart\n","  top_list = current_counter.most_common(10)\n","  x_labels = [val[0] for val in top_list]\n","  y_labels = [val[1] for val in top_list]\n","  plt.figure(figsize=(12, 6))\n","  ax = pd.Series(y_labels).plot(kind='bar')\n","  ax.set_xticklabels(x_labels)\n","\n","  rects = ax.patches\n","\n","  for rect, label in zip(rects, y_labels):\n","      height = rect.get_height()\n","      ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom')\n","  \n","  plt.savefig(\"uncommon-chart-\" + word_level_df['Genre'][i] + \".png\", bbox_inches='tight')\n","  files.download(\"uncommon-chart-\" + word_level_df['Genre'][i] + \".png\")\n","  plt.show()\n","\n","  # draw table\n","  df = pd.DataFrame(top_list, columns=[\"Word\", \"Count\"])\n","  ax = plt.subplot(111, frame_on=False) # no visible frame\n","  ax.xaxis.set_visible(False)  # hide the x axis\n","  ax.yaxis.set_visible(False)  # hide the y axis\n","\n","  table(ax, df, loc='center')  # where df is data frame\n","\n","  plt.savefig(\"uncommon-table-\" + word_level_df['Genre'][i] + \".png\", bbox_inches='tight')\n","  files.download(\"uncommon-table-\" + word_level_df['Genre'][i] + \".png\")\n","\n","  plt.show()\n","  # break\n","\n","  output.clear()\n","output.clear()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-5SnCr6AqLv"},"source":["### Top 10 common words for each tag"]},{"cell_type":"code","metadata":{"id":"nkBOwnpe3wsi","executionInfo":{"status":"ok","timestamp":1620477715122,"user_tz":-270,"elapsed":70255,"user":{"displayName":"Parisa Yalsavar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHdnQtdpJ7oP45jj0RfM96M-L1Zv_aOmk_JZuWfw=s64","userId":"04215397285859188777"}}},"source":["all_relative_normalized_frequency = []\n","\n","for i, group in enumerate(all_groups):\n","  current_group = group.tolist()\n","  other_groups = np.concatenate(np.delete(all_groups, i, axis = 0))#.tolist()\n","\n","  current_counter = Counter(current_group)\n","  other_counter = Counter(other_groups)\n","\n","  current_words = len(list(current_counter))\n","  other_words = len(list(other_counter))\n","\n","  top_list = [(word, (current_counter[word]/current_words) / (other_counter[word]/ other_words)) for word in common_words[i]]\n","  top_list = list(sorted(top_list, key=lambda tup: tup[1], reverse=True))[:10]\n","\n","  all_relative_normalized_frequency.append(top_list)\n","  print(top_list)\n","\n","  # plot chart\n","  x_labels = [val[0] for val in top_list]\n","  y_labels = [val[1] for val in top_list]\n","  plt.figure(figsize=(12, 6))\n","  ax = pd.Series(y_labels).plot(kind='bar')\n","  ax.set_xticklabels(x_labels)\n","\n","  rects = ax.patches\n","\n","  for rect, label in zip(rects, y_labels):\n","      height = rect.get_height()\n","      ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom')\n","  \n","  plt.savefig(\"RelativeNormalizedFrequency-chart-\" + word_level_df['Genre'][i] + \".png\", bbox_inches='tight')\n","  files.download(\"RelativeNormalizedFrequency-chart-\" + word_level_df['Genre'][i] + \".png\")\n","  plt.show()\n","\n","  # draw table\n","  df = pd.DataFrame(top_list, columns=[\"Word\", \"Relative Normalized Frequency\"])\n","  ax = plt.subplot(111, frame_on=False) # no visible frame\n","  ax.xaxis.set_visible(False)  # hide the x axis\n","  ax.yaxis.set_visible(False)  # hide the y axis\n","\n","  table(ax, df, loc='center')  # where df is data frame\n","\n","  plt.savefig(\"RelativeNormalizedFrequency-table-\" + word_level_df['Genre'][i] + \".png\", dpi=500, bbox_inches='tight')\n","  files.download(\"RelativeNormalizedFrequency-table-\" + word_level_df['Genre'][i] + \".png\")\n","  plt.show()\n","\n","\n","  output.clear()\n","output.clear()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sJQCqmXAxEI2"},"source":["### TF-IDF"]},{"cell_type":"code","metadata":{"id":"d3Mxil-Bj04s"},"source":["all_inputs = sentence_level_df['Description'].values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMSCt6fxkTlr"},"source":["all_inputs = [ np.concatenate(x).tolist() for x in all_inputs]\n","all_inputs = [ \" \".join(x) for x in all_inputs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkYyB3EP_Pog"},"source":["cv=CountVectorizer() \n","word_count_vector=cv.fit_transform(np.asarray(all_inputs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Fdiv32EAeak","executionInfo":{"elapsed":7791,"status":"ok","timestamp":1620475327948,"user":{"displayName":"Parisa Yalsavar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHdnQtdpJ7oP45jj0RfM96M-L1Zv_aOmk_JZuWfw=s64","userId":"04215397285859188777"},"user_tz":-270},"outputId":"d55220e2-fdb1-4fc5-ad56-d3637cde9674"},"source":["tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"ft0K3GiYfLTG"},"source":["count_vector=cv.transform(all_inputs) \n","tf_idf_vector=tfidf_transformer.transform(count_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qbmy7-TVx5z6"},"source":["feature_names = cv.get_feature_names() \n"," \n","all_tfidf = []\n","\n","for i in range(tf_idf_vector.shape[0]):\n","  #get tfidf vector for first document \n","  first_document_vector = tf_idf_vector[i]\n","  \n","  #print the scores \n","  df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n","  df.sort_values(by=[\"tfidf\"],ascending=False, inplace=True)\n","\n","  all_tfidf.append(df)\n","  # top_list = df.iloc[:10]\n","\n","  # draw table\n","  df = df.iloc[:10]\n","  ax = plt.subplot(111, frame_on=False) # no visible frame\n","  ax.xaxis.set_visible(False)  # hide the x axis\n","  ax.yaxis.set_visible(False)  # hide the y axis\n","\n","  table(ax, df, loc='center')\n","\n","  plt.savefig(\"tfidf-table-\" + word_level_df['Genre'][i] + \".png\", bbox_inches='tight')\n","  files.download(\"tfidf-table-\" + word_level_df['Genre'][i] + \".png\")\n","\n","  plt.show()\n","  print(\"############\")\n","  print(word_level_df['Genre'][i])\n","  print(all_tfidf[i].iloc[:10])\n","\n","  output.clear()\n","output.clear()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTzMp9BOfT4G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQmgmBUM4hnF"},"source":["### Word frequency histogram"]},{"cell_type":"code","metadata":{"id":"c2Iwnaeylbeh"},"source":["threshold = 20\n","\n","all_groups = word_level_df['Words'].values.tolist()\n","\n","for i in range(len(all_groups)):\n","\n","  word_counter = Counter(all_groups[i])\n","\n","  top_list = word_counter.most_common(threshold)\n","\n","  # plot chart\n","  x_labels = [val[0] for val in top_list]\n","  y_labels = [val[1] for val in top_list]\n","  plt.figure(figsize=(12, 6))\n","  ax = pd.Series(y_labels).plot(kind='bar')\n","  ax.set_xticklabels(x_labels)\n","\n","  rects = ax.patches\n","\n","  for rect, label in zip(rects, y_labels):\n","      height = rect.get_height()\n","      ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom')\n","  \n","  plt.savefig(\"word-frequency-chart-\" + word_level_df['Genre'][i] + \".png\", bbox_inches='tight')\n","  files.download(\"word-frequency-chart-\" + word_level_df['Genre'][i] + \".png\")\n","  plt.show()\n","\n","  # draw table\n","  df = pd.DataFrame(top_list, columns=[\"Word\", \"Relative Normalized mFrequency\"])\n","  ax = plt.subplot(111, frame_on=False) # no visible frame\n","  ax.xaxis.set_visible(False)  # hide the x axis\n","  ax.yaxis.set_visible(False)  # hide the y axis\n","\n","  table(ax, df, loc='center') \n","\n","  plt.savefig(\"word-frequency-table-\" + word_level_df['Genre'][i] + \".png\", dpi=500, bbox_inches='tight')\n","  files.download(\"word-frequency-table-\" + word_level_df['Genre'][i] + \".png\")\n","  plt.show()\n","\n","  output.clear()\n","output.clear()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egIuvYiJKqJO"},"source":[""],"execution_count":null,"outputs":[]}]}